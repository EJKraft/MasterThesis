{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHI Square Test\n",
    "To further look into differences in our data, we conduct multiple chi² tests to see if there are any significant differences between females/males and grad students/PhDs regarding different emotion, affect, level of interest and arousal valence attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import statsmodels.api as sm\n",
    "import sklearn.preprocessing as pp\n",
    "#import nltk\n",
    "import scipy.stats as st\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import Helper as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .csv data with results of OpenSMILE Analysis\n",
    "First we load .csv data and clean it (removing of NaNs), then we store information of all files in seperate panda dataframes containing information about affect, emotion and valence/arousal for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"CHI_2019_FULL.csv\")\n",
    "\n",
    "#Set Labels \n",
    "emotion_label = ['Anger', 'Boredom', 'Disgust', 'Fear', 'Happiness', 'Emo_Neutral', 'Sadness']\n",
    "affect_label = ['Aggressiv', 'Cheerful', 'Intoxicated', 'Nervous', 'Aff_Neutral', 'Tired']\n",
    "loi_label = ['Disinterest', 'Normal', 'High Interest']\n",
    "\n",
    "#Get specific data and save it into new data frames\n",
    "# We use the pandas .copy(deep=True) function to prevent the SettingWithCopyWarning we would otherwise get. Since we do\n",
    "# not write, but only read from the data, the warning does not affect the data frames\n",
    "df_emotion = data[['Anger', 'Boredom', 'Disgust', 'Fear', 'Happiness', 'Emo_Neutral', 'Sadness', 'Filename']].copy(deep=True)\n",
    "df_affect = data[['Aggressiv', 'Cheerful', 'Intoxicated', 'Nervous', 'Aff_Neutral', 'Tired', 'Filename']].copy(deep=True)\n",
    "df_loi = data[['Disinterest', 'Normal', 'High Interest', 'Filename']].copy(deep=True)\n",
    "df_ar_val = data[['Arousal', 'Valence', 'Filename']].copy(deep=True)\n",
    "#For further usage, we want to append the CharacterID as a column, which is saved with other information in the filename\n",
    "#Since we only want the digits, we can remove all non-digit characters of the filename column and append the column to the df\n",
    "\n",
    "df_emotion['Char_ID'] = df_emotion['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_affect['Char_ID'] = df_affect['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_loi['Char_ID'] = df_loi['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_ar_val['Char_ID'] = df_ar_val['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load information about the speakers\n",
    "The speaker ID is saved in a single .csv file containing four important columns: ID, Age, Sex and Acadedmic Status. Since before loaded OpenSMILE csv files are named using the corresponding index (ex. speaker with id 0 has two files 0_a.csv and 0_b.csv), so that a link can be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_data = pd.read_csv(\"CHI_2019_CharacterData.csv\")  \n",
    "\n",
    "#Join above tables and Character Tables\n",
    "\n",
    "#To Join DataFrames we have to cast the column on which we want to join to int, so that both columns have the same data type\n",
    "char_data['ID'] = char_data['ID'].astype(int)\n",
    "df_ar_val['Char_ID'] = df_ar_val['Char_ID'].astype(int)\n",
    "df_emotion['Char_ID'] = df_emotion['Char_ID'].astype(int)\n",
    "df_affect['Char_ID'] = df_affect['Char_ID'].astype(int)\n",
    "df_loi['Char_ID'] = df_loi['Char_ID'].astype(int)\n",
    "\n",
    "#Safe new data frames\n",
    "df_ar_val_char = df_ar_val.merge(char_data, how = 'left', left_on='Char_ID', right_on='ID')\n",
    "df_emotion_char = df_emotion.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "df_affect_char = df_affect.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "df_loi_char = df_loi.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "\n",
    "#Now, we only want to have data containing information about the answers\n",
    "#For that we need to extract from the filename column, whether the file was part of an answer\n",
    "#a = answer, p = presentation, q = question\n",
    "#sentence_type should be the same for all tables, but just to be sure\n",
    "arval_sentence_type = df_ar_val_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_ar_val_char['SentenceType'] = arval_sentence_type\n",
    "emo_sentence_type = df_emotion_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_emotion_char['SentenceType'] = emo_sentence_type\n",
    "aff_sentence_type = df_affect_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_affect_char['SentenceType'] = aff_sentence_type\n",
    "loi_sentence_type = df_loi_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_loi_char['SentenceType'] = loi_sentence_type\n",
    "\n",
    "#Now select only those who have SentenceType == 'a'\n",
    "df_ar_val_char = df_ar_val_char.loc[df_ar_val_char['SentenceType'] == 'q']\n",
    "df_emotion_char = df_emotion_char.loc[df_emotion_char['SentenceType'] == 'q']\n",
    "df_affect_char = df_affect_char.loc[df_affect_char['SentenceType'] == 'q']\n",
    "df_loi_char = df_loi_char.loc[df_loi_char['SentenceType'] == 'q']\n",
    "\n",
    "affect_label.remove('Intoxicated')\n",
    "df_affect_char = df_affect_char.drop(['Intoxicated'], axis = 1)\n",
    "norm_test = pp.normalize(df_affect_char[affect_label], norm = 'l1')\n",
    "df_affect_char[affect_label] = norm_test\n",
    "\n",
    "df_loi_char['Normal Interest'] = df_loi_char['Disinterest'] + df_loi_char['Normal']\n",
    "df_loi_char = df_loi_char.drop(['Disinterest', 'Normal'], axis = 1)\n",
    "loi_label = ['Normal Interest', 'High Interest']\n",
    "\n",
    "#Now let's compensate for multiple samples of the same person\n",
    "IDs = df_emotion_char['Char_ID'].copy()\n",
    "IDs.drop_duplicates()\n",
    "\n",
    "# I know that for loops make the following operations very very slow, but I could not find a way to get this\n",
    "# working with pd.apply and lambda functions... \n",
    "\n",
    "for i in IDs:\n",
    "    #first for emotion\n",
    "    for l in emotion_label:    \n",
    "        emo = df_emotion_char.loc[df_emotion_char.Char_ID == i,l]\n",
    "        emo_neu = hp.constructMedianSeries(emo)\n",
    "        df_emotion_char.loc[df_emotion_char.Char_ID == i,l] = emo_neu\n",
    "    #Then for affect\n",
    "    for l in affect_label:    \n",
    "        aff = df_affect_char.loc[df_affect_char.Char_ID == i,l]\n",
    "        aff_neu = hp.constructMedianSeries(aff)\n",
    "        df_affect_char.loc[df_affect_char.Char_ID == i,l] = aff_neu  \n",
    "    for l in loi_label:    \n",
    "        loi = df_loi_char.loc[df_loi_char.Char_ID == i,l]\n",
    "        loi_neu = hp.constructMedianSeries(loi)\n",
    "        df_loi_char.loc[df_loi_char.Char_ID == i,l] = loi_neu  \n",
    "    for l in ['Arousal', 'Valence']:    \n",
    "        arval = df_ar_val_char.loc[df_ar_val_char.Char_ID == i,l]\n",
    "        arval_neu = hp.constructMedianSeries(arval)\n",
    "        df_ar_val_char.loc[df_ar_val_char.Char_ID == i,l] = arval_neu  \n",
    "        \n",
    "#Now let's drop the duplicate values, since we only need one row now per person\n",
    "df_emotion_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_affect_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_loi_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_ar_val_char.drop_duplicates(subset=['ID'], inplace = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared Test of Independence\n",
    "We Start with characteristic sex. The null hypothesis states that the two categorical variables sex and e.g. emotion are independent.\n",
    "\n",
    "Since we have float data and chi² needs integer data, such as observation counts, we have to convert our data. To illustrate how this is done, we'll look at a specific emotion, 'Anger'. We need to make sure that in our observation count, we do not have any cells with a value of less than 5, since this yields errors and may falsify the result. So we calculate the quantiles of our emotion 'Anger', which yields us three thresholds to compare the float data. This way, we can count how many samples were in the 1st, 2nd, 3rd or 4th Quantile. We want to compare two (or more) groups, so we compare only the female values and sort them into quartiles, then for male values. This yields a 2x4 table. An example table is printed below. This table is used to calculate the chi2 statistic. Note that the function 'calcFrequencyTable' takes in a pd.DataFrame, not a pd.Series and returns an array of pd.DataFrames. This means, that the function calculates these tables for all different emotions defined in e.g. emotion_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Quartile</th>\n",
       "      <th>2nd Quartile</th>\n",
       "      <th>3rd Quartile</th>\n",
       "      <th>4th Quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1st Quartile  2nd Quartile  3rd Quartile  4th Quartile\n",
       "Male              38            33            30            22\n",
       "Female            11            15            18            26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example Frequency Table for the emotion 'Anger':\n",
    "#Since the function does the table calculation for all different emotions, we only want to select the first table\n",
    "#which holds the table for 'anger' (since it's the first element, see declaration of emotion_label at the start)\n",
    "anger_table = hp.calcFrequencyTable(df_emotion_char, emotion_label, 'Sex')[0]\n",
    "anger_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "Chi square of Anger : 11.255254973650317 with p-value of: 0.01042273482234955\n",
      "Chi square of Boredom : 9.951126478465948 with p-value of: 0.018986179581646976\n",
      "Chi square of Disgust : 26.553473160460463 with p-value of: 7.303067977211917e-06\n",
      "Chi square of Fear : 14.326548611625931 with p-value of: 0.002492736690860988\n",
      "Chi square of Happiness : 14.277375026665716 with p-value of: 0.0025509091670906756\n",
      "Chi square of Emo_Neutral : 7.151572783584976 with p-value of: 0.06722038195755599\n",
      "Chi square of Sadness : 28.52162194726904 with p-value of: 2.8224882975886924e-06\n",
      "\n",
      "AFFECT\n",
      "\n",
      "Chi square of Aggressiv : 9.410345409940977 with p-value of: 0.024304512983792822\n",
      "Chi square of Cheerful : 1.1477992881240766 with p-value of: 0.7655494450896176\n",
      "Chi square of Nervous : 6.899576163217902 with p-value of: 0.07516844378225367\n",
      "Chi square of Aff_Neutral : 6.982348775747234 with p-value of: 0.07246251010981195\n",
      "Chi square of Tired : 13.640945605884633 with p-value of: 0.0034369546542939054\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "Chi square of Arousal : 10.445922762252401 with p-value of: 0.015132257235883689\n",
      "Chi square of Valence : 8.792309903055303 with p-value of: 0.03218356598959862\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "Chi square of Normal Interest : 7.804556726871937 with p-value of: 0.050228430916391134\n",
      "Chi square of High Interest : 6.735870329548776 with p-value of: 0.08081036700912533\n",
      "\n",
      "Residuals of: Aff_Neutral\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Quartile</th>\n",
       "      <th>2nd Quartile</th>\n",
       "      <th>3rd Quartile</th>\n",
       "      <th>4th Quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>-1.454414</td>\n",
       "      <td>-1.590033</td>\n",
       "      <td>1.527221</td>\n",
       "      <td>1.527221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>1.454414</td>\n",
       "      <td>1.590033</td>\n",
       "      <td>-1.527221</td>\n",
       "      <td>-1.527221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1st Quartile  2nd Quartile  3rd Quartile  4th Quartile\n",
       "Male       -1.454414     -1.590033      1.527221      1.527221\n",
       "Female      1.454414      1.590033     -1.527221     -1.527221"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "emo_sex_chi2 = hp.chi2(df_emotion_char, emotion_label,'Sex',  True)\n",
    "print('\\nAFFECT\\n')\n",
    "aff_sec_chi2 = hp.chi2(df_affect_char, affect_label,'Sex',  True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "ar_val_sec_chi2 = hp.chi2(df_ar_val_char, ['Arousal', 'Valence'], 'Sex', True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "loi_sec_chi2 = hp.chi2(df_loi_char, ['Normal Interest', 'High Interest'], 'Sex', True)\n",
    "print('\\nResiduals of: '+ affect_label[3])\n",
    "aff_sec_chi2[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the p-values of the emotion of speakers during a question, we see significant results for anger, boredom, disgust, fear, happiness and sadness. Looking at the standardized residuals, we see that for anger females tend to have higher values (4th Quartile ~2.60) than males (1st Quartile ~1.81). Looking at boredom, we see that females tend to have higher values (4th quartile ~2.94) than males (1st Quartile 1.81). Regarding disgust, we see that females tend to have higher values (4th quartile 3.95) than males (1st quartile 3.85). Looking at fear, we see that females have higher values (4th quartile 3.61), but males also have values greater than the median (3rd quartile ~2.04). Also, males have lower values (1st quartile ~1.47), so we can say that females have higher values of fear when asking a question. Regarding happiness, we see that females have only one positive value in the 4th quartile (~3.28), so we can say, that females tend to have a happier voice when asking questions than males. Lastly, if we take a look at the residuals of sadness, we see that females have lower values (1st quartile 4.63 for female), than males (3rd quartile ~1.70; 4th quartile ~3.17), so we can say that males tend to be more sad when asking a question.\n",
    "\n",
    "Now if we have a look at affect, we only see significant p-values for nervous and tired. Regarding nervousness, we see that males tend to be more nervous when asking a question (4th quartile ~2.49) than females (1st quartile ~2.94). Looking at the residuals of tiredness, we see that males tend to be more tired than females. The differences lie between 1st, 2nd and 3rd quartile, where females have a value of ~3.61 for the 1st quartile and males have values of ~2.16 and ~1.02 for the 2nd and 3rd quartile.\n",
    "\n",
    "If we have a look at the p-values for arousal valence, we see that females and males differ significantly regarding arousal. Further looking at the residuals, we see that females tend to have higher arousal values than males (3rd and 4th quartile positive for females; 1st and 2nd quartile positive for males).\n",
    "\n",
    "Looking at the p-values for Level of Interest, we see significant differences in disinterest and high interest. Regarding the residuals we see that females have lower values for disinterest than males (1st and 2nd quartile positive for females, 3rd and 4th quartile positive for males). Looking at high interest we see that females have more values within the 4th quartile (~2.60) and males have more values within the 3rd quartile (~2.04).\n",
    "\n",
    "Now let's look at Native Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "Chi square of Anger : 2.583518923442393 with p-value of: 0.8590059679594921\n",
      "Chi square of Boredom : 8.195645881880656 with p-value of: 0.2241174002296003\n",
      "Chi square of Disgust : 4.902702834205878 with p-value of: 0.5563515417152702\n",
      "Chi square of Fear : 4.80889406368816 with p-value of: 0.5685473126439975\n",
      "Chi square of Happiness : 2.010091040384408 with p-value of: 0.9187681939949454\n",
      "Chi square of Emo_Neutral : 4.46020822883685 with p-value of: 0.6146530581450953\n",
      "Chi square of Sadness : 2.4922595318209217 with p-value of: 0.8693329394224348\n",
      "\n",
      "AFFECT\n",
      "\n",
      "Chi square of Aggressiv : 3.924521482300037 with p-value of: 0.6868900989336226\n",
      "Chi square of Cheerful : 8.773243882166282 with p-value of: 0.1867380065605954\n",
      "Chi square of Nervous : 10.303340774353776 with p-value of: 0.1124452955508459\n",
      "Chi square of Aff_Neutral : 3.698116679467487 with p-value of: 0.7174513896598491\n",
      "Chi square of Tired : 8.455285573650862 with p-value of: 0.20660831857284387\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "Chi square of Arousal : 2.7445463720562473 with p-value of: 0.8401578082878802\n",
      "Chi square of Valence : 11.069738993907855 with p-value of: 0.08624483030865175\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "Chi square of Normal Interest : 4.296218631461801 with p-value of: 0.6366581576824195\n",
      "Chi square of High Interest : 4.417761267333354 with p-value of: 0.6203334384141284\n"
     ]
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "emo_age_chi2 = hp.chi2(df_emotion_char, emotion_label,'IsNativeSpeaker', True)\n",
    "print('\\nAFFECT\\n')\n",
    "aff_age_chi2 = hp.chi2(df_affect_char, affect_label, 'IsNativeSpeaker', True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "ar_val_age_chi2 = hp.chi2(df_ar_val_char, ['Arousal', 'Valence'],'IsNativeSpeaker' ,True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "loi_age_chi2 = hp.chi2(df_loi_char, ['Normal Interest', 'High Interest'], 'IsNativeSpeaker',  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Hoc tests for age and native speaker, as they have three different groups\n",
    "\n",
    "If a significant p-value for the category 'NativeSpeaker' is found, we do not yet know which groups differ significantly from each other, so post-hoc testing is done for this character feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "post-hoc emotions and different groups\n",
      "Anger\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Boredom\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.81713169 0.85802574 1.        ]\n",
      "Disgust\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Fear\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Happiness\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Emo_Neutral\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Sadness\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "\n",
      "AFFECT\n",
      "\n",
      "\n",
      " post-hoc affect and different groups\n",
      "Aggressiv\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Cheerful\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.88760294 1.         1.        ]\n",
      "Nervous\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1.         0.71086187 0.50287887]\n",
      "Aff_Neutral\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Tired\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.76669147 0.9638174  1.        ]\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "\n",
      " post-hoc arousal-valence and different groups\n",
      "Arousal\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Valence\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.28329623 1.         1.        ]\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "\n",
      " post-hoc level of intereset and different groups\n",
      "Normal Interest\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "High Interest\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "print('post-hoc emotions and different groups')\n",
    "emo_reject_list, emo_corrected_p_vals, emo_combinations, emo_residuals= hp.chi2_post_hoc(df_emotion_char,emotion_label, 'IsNativeSpeaker', 'bonferroni', True, True)\n",
    "print('\\nAFFECT\\n')\n",
    "print('\\n post-hoc affect and different groups')\n",
    "aff_reject_list, emo_corrected_p_vals, emo_combinations, aff_residuals = hp.chi2_post_hoc(df_affect_char, affect_label, 'IsNativeSpeaker' ,'bonferroni', True, True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "print('\\n post-hoc arousal-valence and different groups')\n",
    "ar_val_reject_list, ar_val_corrected_p_vals, ar_val_combinations, ar_val_residuals = hp.chi2_post_hoc(df_ar_val_char, ['Arousal', 'Valence'], 'IsNativeSpeaker', 'bonferroni',True, True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "print('\\n post-hoc level of intereset and different groups')\n",
    "loi_reject_list, loi_corrected_p_vals, loi_combinations, loi_residuals = hp.chi2_post_hoc(df_loi_char, loi_label, 'IsNativeSpeaker', 'bonferroni', True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis\n",
    "Now that we know we have significant p-values, we should investigate in which cells the population differs from each other. For this, we can calculate the residuals, which is the difference between the calculated table, and a table cointaining distributed values for which the chi² hypothesis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
