{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHI Square Test\n",
    "To further look into differences in our data, we conduct multiple chi² tests to see if there are any significant differences between females/males and grad students/PhDs regarding different emotion, affect, level of interest and arousal valence attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:49: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import statsmodels.api as sm\n",
    "import sklearn.preprocessing as pp\n",
    "#import nltk\n",
    "import scipy.stats as st\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import Helper as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .csv data with results of OpenSMILE Analysis\n",
    "First we load .csv data and clean it (removing of NaNs), then we store information of all files in seperate panda dataframes containing information about affect, emotion and valence/arousal for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"CHI_2019_FULL.csv\")\n",
    "\n",
    "#Set Labels \n",
    "emotion_label = ['Anger', 'Boredom', 'Disgust', 'Fear', 'Happiness', 'Emo_Neutral', 'Sadness']\n",
    "affect_label = ['Aggressiv', 'Cheerful', 'Intoxicated', 'Nervous', 'Aff_Neutral', 'Tired']\n",
    "loi_label = ['Disinterest', 'Normal', 'High Interest']\n",
    "\n",
    "#Get specific data and save it into new data frames\n",
    "# We use the pandas .copy(deep=True) function to prevent the SettingWithCopyWarning we would otherwise get. Since we do\n",
    "# not write, but only read from the data, the warning does not affect the data frames\n",
    "df_emotion = data[['Anger', 'Boredom', 'Disgust', 'Fear', 'Happiness', 'Emo_Neutral', 'Sadness', 'Filename']].copy(deep=True)\n",
    "df_affect = data[['Aggressiv', 'Cheerful', 'Intoxicated', 'Nervous', 'Aff_Neutral', 'Tired', 'Filename']].copy(deep=True)\n",
    "df_loi = data[['Disinterest', 'Normal', 'High Interest', 'Filename']].copy(deep=True)\n",
    "df_ar_val = data[['Arousal', 'Valence', 'Filename']].copy(deep=True)\n",
    "#For further usage, we want to append the CharacterID as a column, which is saved with other information in the filename\n",
    "#Since we only want the digits, we can remove all non-digit characters of the filename column and append the column to the df\n",
    "\n",
    "df_emotion['Char_ID'] = df_emotion['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_affect['Char_ID'] = df_affect['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_loi['Char_ID'] = df_loi['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "df_ar_val['Char_ID'] = df_ar_val['Filename'].replace('\\D+','', regex = True).copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load information about the speakers\n",
    "The speaker ID is saved in a single .csv file containing four important columns: ID, Age, Sex and Acadedmic Status. Since before loaded OpenSMILE csv files are named using the corresponding index (ex. speaker with id 0 has two files 0_a.csv and 0_b.csv), so that a link can be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_data = pd.read_csv(\"CHI_2019_CharacterData.csv\")  \n",
    "\n",
    "#Join above tables and Character Tables\n",
    "\n",
    "#To Join DataFrames we have to cast the column on which we want to join to int, so that both columns have the same data type\n",
    "char_data['ID'] = char_data['ID'].astype(int)\n",
    "df_ar_val['Char_ID'] = df_ar_val['Char_ID'].astype(int)\n",
    "df_emotion['Char_ID'] = df_emotion['Char_ID'].astype(int)\n",
    "df_affect['Char_ID'] = df_affect['Char_ID'].astype(int)\n",
    "df_loi['Char_ID'] = df_loi['Char_ID'].astype(int)\n",
    "\n",
    "#Safe new data frames\n",
    "df_ar_val_char = df_ar_val.merge(char_data, how = 'left', left_on='Char_ID', right_on='ID')\n",
    "df_emotion_char = df_emotion.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "df_affect_char = df_affect.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "df_loi_char = df_loi.merge(char_data, how = 'left', left_on='Char_ID', right_on= 'ID')\n",
    "\n",
    "#Now, we only want to have data containing information about the answers\n",
    "#For that we need to extract from the filename column, whether the file was part of an answer\n",
    "#a = answer, p = presentation, q = question\n",
    "#sentence_type should be the same for all tables, but just to be sure\n",
    "arval_sentence_type = df_ar_val_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_ar_val_char['SentenceType'] = arval_sentence_type\n",
    "emo_sentence_type = df_emotion_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_emotion_char['SentenceType'] = emo_sentence_type\n",
    "aff_sentence_type = df_affect_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_affect_char['SentenceType'] = aff_sentence_type\n",
    "loi_sentence_type = df_loi_char.Filename.str.replace('\\d+','').str[3:-4]\n",
    "df_loi_char['SentenceType'] = loi_sentence_type\n",
    "\n",
    "#Now select only those who have SentenceType == 'a'\n",
    "df_ar_val_char = df_ar_val_char.loc[df_ar_val_char['SentenceType'] == 'q']\n",
    "df_emotion_char = df_emotion_char.loc[df_emotion_char['SentenceType'] == 'q']\n",
    "df_affect_char = df_affect_char.loc[df_affect_char['SentenceType'] == 'q']\n",
    "df_loi_char = df_loi_char.loc[df_loi_char['SentenceType'] == 'q']\n",
    "\n",
    "affect_label.remove('Intoxicated')\n",
    "df_affect_char = df_affect_char.drop(['Intoxicated'], axis = 1)\n",
    "norm_test = pp.normalize(df_affect_char[affect_label], norm = 'l1')\n",
    "df_affect_char[affect_label] = norm_test\n",
    "\n",
    "df_loi_char['Normal Interest'] = df_loi_char['Disinterest'] + df_loi_char['Normal']\n",
    "df_loi_char = df_loi_char.drop(['Disinterest', 'Normal'], axis = 1)\n",
    "loi_label = ['Normal Interest', 'High Interest']\n",
    "\n",
    "#Now let's compensate for multiple samples of the same person\n",
    "IDs = df_emotion_char['Char_ID'].copy()\n",
    "IDs.drop_duplicates()\n",
    "\n",
    "# I know that for loops make the following operations very very slow, but I could not find a way to get this\n",
    "# working with pd.apply and lambda functions... \n",
    "\n",
    "for i in IDs:\n",
    "    #first for emotion\n",
    "    for l in emotion_label:    \n",
    "        emo = df_emotion_char.loc[df_emotion_char.Char_ID == i,l]\n",
    "        emo_neu = hp.constructMedianSeries(emo)\n",
    "        df_emotion_char.loc[df_emotion_char.Char_ID == i,l] = emo_neu\n",
    "    #Then for affect\n",
    "    for l in affect_label:    \n",
    "        aff = df_affect_char.loc[df_affect_char.Char_ID == i,l]\n",
    "        aff_neu = hp.constructMedianSeries(aff)\n",
    "        df_affect_char.loc[df_affect_char.Char_ID == i,l] = aff_neu  \n",
    "    for l in loi_label:    \n",
    "        loi = df_loi_char.loc[df_loi_char.Char_ID == i,l]\n",
    "        loi_neu = hp.constructMedianSeries(loi)\n",
    "        df_loi_char.loc[df_loi_char.Char_ID == i,l] = loi_neu  \n",
    "    for l in ['Arousal', 'Valence']:    \n",
    "        arval = df_ar_val_char.loc[df_ar_val_char.Char_ID == i,l]\n",
    "        arval_neu = hp.constructMedianSeries(arval)\n",
    "        df_ar_val_char.loc[df_ar_val_char.Char_ID == i,l] = arval_neu  \n",
    "        \n",
    "#Now let's drop the duplicate values, since we only need one row now per person\n",
    "df_emotion_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_affect_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_loi_char.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_ar_val_char.drop_duplicates(subset=['ID'], inplace = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared Test of Independence\n",
    "We Start with characteristic sex. The null hypothesis states that the two categorical variables sex and e.g. emotion are independent.\n",
    "\n",
    "Since we have float data and chi² needs integer data, such as observation counts, we have to convert our data. To illustrate how this is done, we'll look at a specific emotion, 'Anger'. We need to make sure that in our observation count, we do not have any cells with a value of less than 5, since this yields errors and may falsify the result. So we calculate the quantiles of our emotion 'Anger', which yields us three thresholds to compare the float data. This way, we can count how many samples were in the 1st, 2nd, 3rd or 4th Quantile. We want to compare two (or more) groups, so we compare only the female values and sort them into quartiles, then for male values. This yields a 2x4 table. An example table is printed below. This table is used to calculate the chi2 statistic. Note that the function 'calcFrequencyTable' takes in a pd.DataFrame, not a pd.Series and returns an array of pd.DataFrames. This means, that the function calculates these tables for all different emotions defined in e.g. emotion_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Quartile</th>\n",
       "      <th>2nd Quartile</th>\n",
       "      <th>3rd Quartile</th>\n",
       "      <th>4th Quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1st Quartile 2nd Quartile 3rd Quartile 4th Quartile\n",
       "Male             36           33           31           22\n",
       "Female           12           14           16           26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example Frequency Table for the emotion 'Anger':\n",
    "#Since the function does the table calculation for all different emotions, we only want to select the first table\n",
    "#which holds the table for 'anger' (since it's the first element, see declaration of emotion_label at the start)\n",
    "anger_table = hp.calcFrequencyTable(df_emotion_char, emotion_label, 'Sex')[0]\n",
    "anger_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "Chi square of Anger : 10.284812128548662 with p-value of: 0.016293998327015533\n",
      "Cramers V: 0.23265981636341096\n",
      "Chi square of Boredom : 10.284812128548662 with p-value of: 0.016293998327015533\n",
      "Cramers V: 0.23265981636341096\n",
      "Chi square of Disgust : 22.691201262507096 with p-value of: 4.6832435844157946e-05\n",
      "Cramers V: 0.3455826602505207\n",
      "Chi square of Fear : 14.657524219140045 with p-value of: 0.002133988461169485\n",
      "Cramers V: 0.27774964322436746\n",
      "Chi square of Happiness : 12.589792696813639 with p-value of: 0.005613151619097775\n",
      "Cramers V: 0.2574141931310994\n",
      "Chi square of Emo_Neutral : 7.505333468406547 with p-value of: 0.05742157060635065\n",
      "Cramers V: 0.19875048451985358\n",
      "Chi square of Sadness : 29.210727890737736 with p-value of: 2.0223309349936657e-06\n",
      "Cramers V: 0.3920977851367513\n",
      "\n",
      "AFFECT\n",
      "\n",
      "Chi square of Aggressiv : 9.173792206104626 with p-value of: 0.027067276480438007\n",
      "Cramers V: 0.21973419596010738\n",
      "Chi square of Cheerful : 1.1015380820971568 with p-value of: 0.7767027974990817\n",
      "Cramers V: 0.07614176812461693\n",
      "Chi square of Nervous : 6.724147585438082 with p-value of: 0.08122968618018123\n",
      "Cramers V: 0.1881229663004337\n",
      "Chi square of Aff_Neutral : 5.875451811348886 with p-value of: 0.11782952218090999\n",
      "Cramers V: 0.17585059165680042\n",
      "Chi square of Tired : 13.48478096767134 with p-value of: 0.003697344214592976\n",
      "Cramers V: 0.2664067030102593\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "Chi square of Arousal : 9.409112398011175 with p-value of: 0.02431817042173693\n",
      "Cramers V: 0.22253459101914652\n",
      "Chi square of Valence : 7.759942200633304 with p-value of: 0.051242462466911054\n",
      "Cramers V: 0.20209354530999935\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "Chi square of Normal Interest : 5.591910268641814 with p-value of: 0.13324355323897208\n",
      "Cramers V: 0.17155496692623673\n",
      "Chi square of High Interest : 5.591910268641813 with p-value of: 0.133243553238972\n",
      "Cramers V: 0.17155496692623673\n",
      "\n",
      "Residuals of: Aff_Neutral\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Quartile</th>\n",
       "      <th>2nd Quartile</th>\n",
       "      <th>3rd Quartile</th>\n",
       "      <th>4th Quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>-1.330804</td>\n",
       "      <td>-1.465703</td>\n",
       "      <td>1.340177</td>\n",
       "      <td>1.455453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>1.330804</td>\n",
       "      <td>1.465703</td>\n",
       "      <td>-1.340177</td>\n",
       "      <td>-1.455453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1st Quartile  2nd Quartile  3rd Quartile  4th Quartile\n",
       "Male       -1.330804     -1.465703      1.340177      1.455453\n",
       "Female      1.330804      1.465703     -1.340177     -1.455453"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "emo_sex_chi2 = hp.chi2(df_emotion_char, emotion_label,'Sex',  True)\n",
    "print('\\nAFFECT\\n')\n",
    "aff_sec_chi2 = hp.chi2(df_affect_char, affect_label,'Sex',  True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "ar_val_sec_chi2 = hp.chi2(df_ar_val_char, ['Arousal', 'Valence'], 'Sex', True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "loi_sec_chi2 = hp.chi2(df_loi_char, ['Normal Interest', 'High Interest'], 'Sex', True)\n",
    "print('\\nResiduals of: '+ affect_label[3])\n",
    "aff_sec_chi2[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the p-values of the emotion of speakers during a question, we see significant results for anger, boredom, disgust, fear, happiness and sadness. Looking at the standardized residuals, we see that for anger females tend to have higher values (4th Quartile ~2.60) than males (1st Quartile ~1.81). Looking at boredom, we see that females tend to have higher values (4th quartile ~2.94) than males (1st Quartile 1.81). Regarding disgust, we see that females tend to have higher values (4th quartile 3.95) than males (1st quartile 3.85). Looking at fear, we see that females have higher values (4th quartile 3.61), but males also have values greater than the median (3rd quartile ~2.04). Also, males have lower values (1st quartile ~1.47), so we can say that females have higher values of fear when asking a question. Regarding happiness, we see that females have only one positive value in the 4th quartile (~3.28), so we can say, that females tend to have a happier voice when asking questions than males. Lastly, if we take a look at the residuals of sadness, we see that females have lower values (1st quartile 4.63 for female), than males (3rd quartile ~1.70; 4th quartile ~3.17), so we can say that males tend to be more sad when asking a question.\n",
    "\n",
    "Now if we have a look at affect, we only see significant p-values for nervous and tired. Regarding nervousness, we see that males tend to be more nervous when asking a question (4th quartile ~2.49) than females (1st quartile ~2.94). Looking at the residuals of tiredness, we see that males tend to be more tired than females. The differences lie between 1st, 2nd and 3rd quartile, where females have a value of ~3.61 for the 1st quartile and males have values of ~2.16 and ~1.02 for the 2nd and 3rd quartile.\n",
    "\n",
    "If we have a look at the p-values for arousal valence, we see that females and males differ significantly regarding arousal. Further looking at the residuals, we see that females tend to have higher arousal values than males (3rd and 4th quartile positive for females; 1st and 2nd quartile positive for males).\n",
    "\n",
    "Looking at the p-values for Level of Interest, we see significant differences in disinterest and high interest. Regarding the residuals we see that females have lower values for disinterest than males (1st and 2nd quartile positive for females, 3rd and 4th quartile positive for males). Looking at high interest we see that females have more values within the 4th quartile (~2.60) and males have more values within the 3rd quartile (~2.04).\n",
    "\n",
    "Now let's look at Native Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "Chi square of Anger : 4.342077728866063 with p-value of: 0.63048927388812\n",
      "Cramers V: 0.15117222191342708\n",
      "Chi square of Boredom : 10.460252234993616 with p-value of: 0.10656045014808764\n",
      "Cramers V: 0.23463580104396975\n",
      "Chi square of Disgust : 4.1684854351783915 with p-value of: 0.6538875933996489\n",
      "Cramers V: 0.14811953627815166\n",
      "Chi square of Fear : 3.4763969729083457 with p-value of: 0.7471072827144176\n",
      "Cramers V: 0.13526576127461754\n",
      "Chi square of Happiness : 2.5841201665715605 with p-value of: 0.8589370416199453\n",
      "Cramers V: 0.11662174949656601\n",
      "Chi square of Emo_Neutral : 4.912754069182902 with p-value of: 0.5550508910592722\n",
      "Cramers V: 0.1607998767540589\n",
      "Chi square of Sadness : 2.93483553436048 with p-value of: 0.816980125698439\n",
      "Cramers V: 0.12428396039885267\n",
      "\n",
      "AFFECT\n",
      "\n",
      "Chi square of Aggressiv : 4.638318014456129 with p-value of: 0.5909646112920222\n",
      "Cramers V: 0.15624404012980747\n",
      "Chi square of Cheerful : 10.505598013640933 with p-value of: 0.10491211142506546\n",
      "Cramers V: 0.23514383071734193\n",
      "Chi square of Nervous : 11.284404533844189 with p-value of: 0.07997435109353081\n",
      "Cramers V: 0.24370392448564765\n",
      "Chi square of Aff_Neutral : 4.649549264965626 with p-value of: 0.5894798297358792\n",
      "Cramers V: 0.15643309087552965\n",
      "Chi square of Tired : 10.102120356783782 with p-value of: 0.12041686297816558\n",
      "Cramers V: 0.23058415927679926\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "Chi square of Arousal : 4.843340126626993 with p-value of: 0.5640574234849134\n",
      "Cramers V: 0.15965983785646468\n",
      "Chi square of Valence : 10.588156062335264 with p-value of: 0.10196991531943357\n",
      "Cramers V: 0.2360659593634491\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "Chi square of Normal Interest : 3.637205272954539 with p-value of: 0.7256346187238556\n",
      "Cramers V: 0.13835890158254782\n",
      "Chi square of High Interest : 3.6372052729545388 with p-value of: 0.7256346187238556\n",
      "Cramers V: 0.13835890158254782\n"
     ]
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "emo_age_chi2 = hp.chi2(df_emotion_char, emotion_label,'IsNativeSpeaker', True)\n",
    "print('\\nAFFECT\\n')\n",
    "aff_age_chi2 = hp.chi2(df_affect_char, affect_label, 'IsNativeSpeaker', True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "ar_val_age_chi2 = hp.chi2(df_ar_val_char, ['Arousal', 'Valence'],'IsNativeSpeaker' ,True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "loi_age_chi2 = hp.chi2(df_loi_char, ['Normal Interest', 'High Interest'], 'IsNativeSpeaker',  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Hoc tests for age and native speaker, as they have three different groups\n",
    "\n",
    "If a significant p-value for the category 'NativeSpeaker' is found, we do not yet know which groups differ significantly from each other, so post-hoc testing is done for this character feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "\n",
      "post-hoc emotions and different groups\n",
      "Anger\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Boredom\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.55024625 0.48248592 1.        ]\n",
      "Disgust\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Fear\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Happiness\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Emo_Neutral\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Sadness\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "\n",
      "AFFECT\n",
      "\n",
      "\n",
      " post-hoc affect and different groups\n",
      "Aggressiv\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Cheerful\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.66154082 0.72545054 1.        ]\n",
      "Nervous\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1.         0.83574532 0.34767393]\n",
      "Aff_Neutral\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Tired\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.72703881 0.6732205  1.        ]\n",
      "\n",
      "AROUSAL-VALENCE\n",
      "\n",
      "\n",
      " post-hoc arousal-valence and different groups\n",
      "Arousal\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "Valence\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [0.35026169 1.         1.        ]\n",
      "\n",
      "LEVEL OF INTEREST\n",
      "\n",
      "\n",
      " post-hoc level of intereset and different groups\n",
      "Normal Interest\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n",
      "High Interest\n",
      "Combinations: [('Asian Non-Native', 'Europ. Non-Native'), ('Asian Non-Native', 'Native Speaker'), ('Europ. Non-Native', 'Native Speaker')]\n",
      "Reject List: [False False False]\n",
      "Corrected p-values: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('EMOTION\\n')\n",
    "print('post-hoc emotions and different groups')\n",
    "emo_reject_list, emo_corrected_p_vals, emo_combinations, emo_residuals= hp.chi2_post_hoc(df_emotion_char,emotion_label, 'IsNativeSpeaker', 'bonferroni', True, True)\n",
    "print('\\nAFFECT\\n')\n",
    "print('\\n post-hoc affect and different groups')\n",
    "aff_reject_list, emo_corrected_p_vals, emo_combinations, aff_residuals = hp.chi2_post_hoc(df_affect_char, affect_label, 'IsNativeSpeaker' ,'bonferroni', True, True)\n",
    "print('\\nAROUSAL-VALENCE\\n')\n",
    "print('\\n post-hoc arousal-valence and different groups')\n",
    "ar_val_reject_list, ar_val_corrected_p_vals, ar_val_combinations, ar_val_residuals = hp.chi2_post_hoc(df_ar_val_char, ['Arousal', 'Valence'], 'IsNativeSpeaker', 'bonferroni',True, True)\n",
    "print('\\nLEVEL OF INTEREST\\n')\n",
    "print('\\n post-hoc level of intereset and different groups')\n",
    "loi_reject_list, loi_corrected_p_vals, loi_combinations, loi_residuals = hp.chi2_post_hoc(df_loi_char, loi_label, 'IsNativeSpeaker', 'bonferroni', True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis\n",
    "Now that we know we have significant p-values, we should investigate in which cells the population differs from each other. For this, we can calculate the residuals, which is the difference between the calculated table, and a table cointaining distributed values for which the chi² hypothesis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
